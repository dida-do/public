{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents import tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "class WikipediaQuery(BaseModel):\n",
    "    query: str = Field(description=\"The query to search on wikipedia.\")\n",
    "    \n",
    "class RestaurantSearchQuery(BaseModel):\n",
    "    cuisine: str = Field(description=\"The cuisine to search for.\")\n",
    "    location: str = Field(description=\"The location to search for restaurants.\",default=\"Berlin\")\n",
    "    \n",
    "@tool(args_schema=WikipediaQuery)\n",
    "def wiki_search(query: str) -> str:\n",
    "    \"\"\"Search Wikipedia for a query.\"\"\"\n",
    "    try:\n",
    "        page_titles = wikipedia.search(query)\n",
    "    except:\n",
    "        return \"It is currently busy.\"\n",
    "    \n",
    "    if len(page_titles) == 0:\n",
    "        return \"No Wikipedia search results found.\"\n",
    "\n",
    "    summaries = []\n",
    "    for page_title in page_titles[:2]:\n",
    "        try:\n",
    "            wiki_page = wikipedia.page(page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Title: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No Wikipedia search results found.\"\n",
    "    return \"\"\"\\n\\n-------------------------------PAGE-------------------------------\\n\\n\"\"\".join(summaries)\n",
    "\n",
    "@tool(args_schema=RestaurantSearchQuery)\n",
    "def restaurant_search(cuisine: str,location: str=\"Berlin\") -> str:\n",
    "    \"\"\"Search for restaurants in a location and cuisine.\"\"\"\n",
    "    return f\"Searching for {cuisine} restaurants in {location}. You can find the best {cuisine} restaurants in {location} by going in 500m to the left and then 200m to the right. The name of the restaurant is 'The Best {cuisine} Restaurant'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wiki_search(\"Berlin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(restaurant_search(\"vietnamese\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formating function for Openai\n",
    "\n",
    "the format works also on other models like Mixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_tool_to_openai_function(wiki_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model with function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0,api_key=os.getenv(\"OPENAI_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_function = llm.bind(functions=[format_tool_to_openai_function(func) for func in [wiki_search,restaurant_search]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chattemplate\n",
    "Please look to the link below to understand, how I build `ChatPromptTemplate`\n",
    "https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{question}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm_with_function | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"question\": \"Where is Berlin?\",\"agent_scratchpad\":[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the output here ie. tool and tool response are `intermediate_steps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_response = wiki_search(output.tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tool_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({\"question\": \"Where can I find a good vietnamese reustaurant in Berlin\",\"agent_scratchpad\":[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(restaurant_search(output.tool_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `function_messages` is `intermediate_steps` response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_messages=format_to_openai_functions([(output, restaurant_search(output.tool_input))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the ``AIMessage`` and ``FunctionMessage`` back to `agent_scratchpad`, they are the response from intermediate  step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_finish = chain.invoke({\n",
    "    \"question\": \"Where can I find a good vietnamese reustaurant in Berlin\",\n",
    "    \"agent_scratchpad\": function_messages\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_finish.return_values[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain = (RunnableParallel({\"question\": itemgetter(\"question\"),\n",
    "                          \"agent_scratchpad\":itemgetter(\"intermediate_steps\") | RunnableLambda(format_to_openai_functions)}) | \n",
    "                          prompt | \n",
    "                          llm_with_function | \n",
    "                          OpenAIFunctionsAgentOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent_chain, tools=[wiki_search,restaurant_search], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"question\": \"Where can I find a good vietnamese reustaurant in Berlin\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True,\n",
    "                                  memory_key=\"history\",\n",
    "                                  input_key=\"question\", # key name from input to add to memory\n",
    "                                  output_key=\"output\")# key name from output to add to memory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{question}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "agent_chain_with_memory = (RunnableParallel({\"question\":itemgetter(\"question\"),\n",
    "                            \"agent_scratchpad\":itemgetter(\"intermediate_steps\") | RunnableLambda(format_to_openai_functions),\n",
    "                            \"history\":itemgetter(\"history\")}) |\n",
    "                prompt | \n",
    "                llm_with_function | \n",
    "                OpenAIFunctionsAgentOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_memory = AgentExecutor(agent=agent_chain_with_memory, tools=[wiki_search,restaurant_search], verbose=False,memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=agent_with_memory.invoke({\"question\": \"Hello, my name is Long.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=agent_with_memory.invoke({\"question\": \"What is my name?\"})\n",
    "print(x[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_with_memory.invoke({\"question\": \"What are the things that Berlin is famous for?\"})[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tavily\n",
    "\n",
    "for using tavily you need to run:\n",
    "\n",
    "`pip install tavily-python`\n",
    "\n",
    "You can create an free account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forcing function call with function_call={\"name\":\"tavily_search\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tav = TavilySearchResults(max_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tav(\"What are the current news from nvidia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TavilyQuery(BaseModel):\n",
    "    query: str = Field(description=\"The query to search for news.\")\n",
    "    \n",
    "@tool(args_schema=TavilyQuery)\n",
    "def tavily_search(query: str) -> str:\n",
    "    \"\"\"Search news on tavily.\"\"\"\n",
    "    tavily = TavilySearchResults(max_results=3)\n",
    "    docs = tavily(query)\n",
    "    if not docs:\n",
    "        return \"No news found.\"\n",
    "    return \"\"\"\\n\\n-------------------------------PAGE-------------------------------\\n\\n\"\"\".join([doc[\"content\"] for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True,\n",
    "                                  memory_key=\"history\",\n",
    "                                  input_key=\"question\", # key name from input to add to memory\n",
    "                                  output_key=\"output\")# key name from output to add to memory\n",
    "agent_chain_with_memory = (RunnableParallel({\"question\":itemgetter(\"question\"),\n",
    "                            \"agent_scratchpad\":itemgetter(\"intermediate_steps\") | RunnableLambda(format_to_openai_functions),\n",
    "                            \"history\":itemgetter(\"history\")}) |\n",
    "                prompt | \n",
    "                llm.bind(functions=[format_tool_to_openai_function(func) for func in [tavily_search, wiki_search]]) | \n",
    "                OpenAIFunctionsAgentOutputParser())\n",
    "agent_with_memory = AgentExecutor(agent=agent_chain_with_memory, tools=[tavily_search,wiki_search], verbose=True,memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_with_memory.invoke({\"question\": \"What are the current news from nvidia about their share?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_with_memory.invoke({\"question\": \"What are the thing that Berlin are famous for?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

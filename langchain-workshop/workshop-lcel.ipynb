{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First to work with LLM, you need to understand this:\n",
    "\n",
    "LLM requires a ```chat_template```, if you do not use the model's specific template, then it would not generate the tokens as you expected.\n",
    "\n",
    "For example have a look to ```meta-llama/Meta-Llama-3-8B-Instruct```, to run the cell belows you need an ```api_token``` from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "                                          token=os.getenv(\"HF_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is\n",
    "```\n",
    "\"\"\"\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "What is the weather like?<|im_end|>\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "So this would be an input prompt for ```meta-llama/Meta-Llama-3-8B-Instruct```. In this case it add 2 tokens ```<|im_start|>system\\n``` and ```<|im_end|>``` around the system message, ```<|im_start|>user\\n``` and ```<|im_end|>``` around the user message to indicate the system message and user message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    tokenizer.apply_chat_template([\n",
    "        {\"role\":\"system\",\n",
    "        \"content\":\"You are a helpful assistant.\"},\n",
    "        {\"role\":\"user\",\n",
    "        \"content\":\"What is the weather like?\"}],\n",
    "    add_generation_prompt=False,\n",
    "    tokenize=False \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can futher add more message to see how it works, by adding more messages you will ad a chat memory to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    tokenizer.apply_chat_template([\n",
    "    {\"role\":\"system\",\n",
    "    \"content\":\"You are a helpful assistant.\"},\n",
    "    {\"role\":\"user\",\n",
    "    \"content\":\"What is the weather like?\"},\n",
    "    {\"role\":\"assistant\",\n",
    "     \"content\":\"It is sunny today.\"},\n",
    "    {\"role\":\"user\",\n",
    "    \"content\":\"And how are you doing?\"},\n",
    "    ],add_generation_prompt=False,\n",
    "    tokenize=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In background if you are using OpenAI or any LLMs from any providers, they will apply the template for you, you need only specify the roles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### invoke\n",
    "\n",
    "calls the chain on a single input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# parser convert a langchain AIMessage or output of an lanmgchain LLM to a string, if it is already a string it will return it as is.\n",
    "parser = StrOutputParser()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",\n",
    "                 temperature=0,\n",
    "                 api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# This will create a chat prompt template for `user`-role.\n",
    "prompt = ChatPromptTemplate.from_template(\"{question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"What is the capital of Germany?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling `invoke` the prompt will return a ``HumanMessage``, this is similar to the `user`-role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.invoke(\"What is the capital of Germany?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm \n",
    "\n",
    "chain.invoke({\"question\":\"What is the capital of Germany?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"question\":\"What is the capital of Germany?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### streaming\n",
    "\n",
    "Every LangChain components and chain have the ``stream``-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chain.stream({\"question\":\"Write a poem about water.\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specialcase of Runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prompt will require 2 inputs, `question` and `answer`.\n",
    "test_prompt = ChatPromptTemplate.from_template(\"\"\"Here is the question: {question}\n",
    "                                               Here is the answer: {answer}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dictionary as component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can chain a dictionary with a LangChain component.\n",
    "# let consider the following problem:\n",
    "# our test_prompt requires a dictionary with keys `question` and `answer` and we want to pass a dictionary with keys `a` and `b`.\n",
    "({\"question\":lambda x: x[\"a\"],\n",
    "  \"answer\": lambda x: x[\"b\"]} | test_prompt).invoke({\"a\":\"What is the capital of Germany?\",\"b\":\"Berlin\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function as a component\n",
    "\n",
    "I would recommend to use \n",
    "\n",
    "```\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "```\n",
    "to wrap around your function so it would also have `Ã¬nvoke`, `stream`\n",
    "\n",
    "but for now let see what happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_string_parsing(input_prompt)->str:\n",
    "    \"\"\"Convert LangChain ChatPromptValue to a string.\"\"\"\n",
    "    return input_prompt.messages[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "({\"question\":lambda x: x[\"a\"],\n",
    "  \"answer\": lambda x: x[\"b\"]} | \n",
    "  test_prompt | \n",
    "  test_string_parsing).invoke({\"a\":\"What is the capital of Germany?\",\"b\":\"Berlin\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiple inputs\n",
    "\n",
    "If your function requires multiple arguments, then you have to pass the arguments as a dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func1(input_dict:dict):\n",
    "    question = input_dict[\"question\"]\n",
    "    answer = input_dict[\"answer\"]\n",
    "    return {\"question\":\"hier is the question: \"+question, \"answer\":\"here is the answer: \"+answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_func1 | test_prompt).invoke({\"question\":\"What is the capital of Germany?\",\"answer\":\"Berlin\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding config to function\n",
    "\n",
    "If you have a `config` in the function argument, langchain will pass the runtime config to this argument!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func2(input_dict:dict,config:dict):\n",
    "    print(config)\n",
    "    question = input_dict[\"question\"]\n",
    "    answer = input_dict[\"answer\"]\n",
    "    return {\"question\":\"hier is the question: \"+question, \"answer\":\"here is the answer: \"+answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_func2 | test_prompt).invoke({\"question\":\"What is the capital of Germany?\",\"answer\":\"Berlin\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_func2 | test_prompt).invoke({\"question\":\"What is the capital of Germany?\",\"answer\":\"Berlin\"},{\"tags\": [\"my-tag\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example Image](pics/rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_os()->str:\n",
    "    \"\"\"Check the OS of the system\"\"\"\n",
    "    os_name = platform.system()\n",
    "    return os_name\n",
    "if check_os() == \"Windows\":\n",
    "    embedding_path=\"huggingface_models\\\\BAAI\\\\bge-large-en-v1.5\"\n",
    "else:\n",
    "    embedding_path=\"huggingface_models/BAAI/bge-large-en-v1.5\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING VECTOR DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `search_type` you have the following 2 options:\n",
    "\n",
    "- `similarity`: ranking by the score, in this case the cosine-similarity score.\n",
    "\n",
    "- `mmr`: Maximal Marginal Relevance\n",
    "\n",
    "select examples based on a combination of which examples are most similar to the input query, while optimizing for diversity\n",
    "\n",
    "        `$MMR = arg\\ max_{D_i\\in R\\setminus S} [\\lambda \\ cosin(D_i,Q) - (1-\\lambda)\\ max_{D_J\\in S} cosin(D_i,D_j)]$`\n",
    "\n",
    "- `k`: Number of returned documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_db = Qdrant.from_existing_collection(embedding=embeddings,\n",
    "                                         path=\"./qdrant-database\",\n",
    "                                        collection_name=\"llm_papers\")\n",
    "retriever = qdrant_db.as_retriever(search_type=\"similarity\",search_kwargs={'k': 5,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"How does self-rag work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADDING CONTEXT TO USER PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this template will require a dictionary with keys `question` and `context`\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Context: ```{context}```\n",
    "\"\"\"\n",
    "def formatting_page_content(page_contents)->str:\n",
    "    \"\"\"Convert a list of langchain Document to a string.\"\"\"\n",
    "    context = \"\"\n",
    "    for page in page_contents:\n",
    "        context += \"\\n--------------NEW DOCUMENT----------------\\n\"+page.page_content\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using `from_messages` method to create a ChatPromptTemplate from a list of messages.\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\"\\\n",
    "                  \" If you don't know the answer, just say that you don't know.\"\\\n",
    "                \" You must answer the question using only the provided context and do not include any information from outside the given context.\"),\n",
    "    (\"human\",template)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel\n",
    "\n",
    "Run the chain in parallel instead of sequential. So if you would use multiple LLMs, it will call the LLMs in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_chain = RunnableParallel({\n",
    "    \"context\": retriever | formatting_page_content,\n",
    "    \"question\": RunnablePassthrough()\n",
    "})| rag_prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example Image](pics/rag2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG STREAMING EDXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in simple_rag_chain.stream(\"How does self-rag work?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagReranker\n",
    "if check_os() == \"Windows\":\n",
    "    ranker_path=\"huggingface_models\\\\BAAI\\\\bge-reranker-large\"\n",
    "else:\n",
    "    ranker_path=\"huggingface_models/BAAI/bge-reranker-large\"\n",
    "reranker = FlagReranker(ranker_path, use_fp16=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def ranking_documents(input_dict:dict,\n",
    "                      k:int=3,\n",
    "                      query_key:str=\"question\",\n",
    "                      documents_key:str=\"documents\")->list:\n",
    "    \"\"\"Ranking the documents based on the scores computed by the reranker model.\"\"\"\n",
    "    \n",
    "    query = input_dict[query_key]\n",
    "    documents = input_dict[documents_key]\n",
    "    # compute scores between query and documents\n",
    "    scores = [reranker.compute_score([query, doc.page_content]) for doc in documents]\n",
    "    # ranking and sorting documents by scores\n",
    "    zip_docs= list(zip(documents,scores))\n",
    "    zip_docs.sort(key=lambda x: x[1],reverse=True)\n",
    "    # retrun top k documents by scores\n",
    "    return [doc[0] for doc in zip_docs][:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set configureable\n",
    "\n",
    "First we need to introduce to `configurable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```k``` is number of retrieved documents from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrant_db.as_retriever(search_type=\"similarity\",\n",
    "                                   search_kwargs={'k': 5,}).configurable_fields(\n",
    "    # make search_kwargs field configurable\n",
    "    search_kwargs=ConfigurableField(\n",
    "        id=\"retriever_kwargs\",\n",
    "        name=\"retriever_kwargs\",\n",
    "        description=\"Return number of trevied documents\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can change the number of retrieved documents by changing the value of `retriever_kwargs` field.\n",
    "# we have the following 2 options to change the value of `retriever_kwargs` field.\n",
    "docs = retriever.with_config(configurable={\"retriever_kwargs\": {\"k\":3}}).invoke(\"How does self-rag work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"How does self-rag work?\",config={\"configurable\": {\"retriever_kwargs\": {\"k\":7}}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define chain with ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = RunnableLambda(partial(ranking_documents,\n",
    "                                query_key=\"question\",\n",
    "                                documents_key=\"documents\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker.invoke({\"question\":\"How does self-rag work?\",\"documents\":docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_with_ranker = RunnableParallel({\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough()\n",
    "})| RunnableParallel({\n",
    "    \"context\": ranker|formatting_page_content,\n",
    "    \"question\": itemgetter(\"question\")}) | rag_prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example Image](pics/rag3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the graph can be also generated by calling `get_graph` method.\n",
    "rag_with_ranker.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_with_ranker.invoke(\"How does self-rag work?\",{\"configurable\": {\"retriever_kwargs\": {\"k\":5}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_with_ranker.with_config(configurable={\"retriever_kwargs\": {\"k\":5}}).invoke(\"How does self-rag work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# MessagesPlaceholder is a placeholder for a list of messages.\n",
    "\n",
    "# I have to change the system message to make the LLM not to use the context.\n",
    "rag_prompt_With_memory = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(\"You are a helpful assistant. Use the context given from the user to answer the question if needed. Otherwise you can ignore it.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"), # a placeholder for the history\n",
    "    (\"human\",template)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example on placeholder\n",
    "It requires the key `history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_With_memory.invoke({\"history\":[],\n",
    "                               \"context\":\"This is the context\",\n",
    "                               \"question\":\"This is the question\"}).messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_With_memory.invoke({\"history\":[HumanMessage(\"hello\"),\n",
    "                                          AIMessage(\"hi\")],\n",
    "                               \"context\":\"This is the context\",\n",
    "                               \"question\":\"This is the question\"}).messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLY MEMORY TO RANKER PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_memory_with_ranker = RunnableParallel({\n",
    "    \"documents\": itemgetter(\"input\")|retriever,\n",
    "    \"question\": itemgetter(\"input\"),\n",
    "    \"history\": itemgetter(\"history\"),\n",
    "}) | RunnableParallel({\n",
    "    \"context\": ranker|formatting_page_content,\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    \"history\":itemgetter(\"history\")}) | rag_prompt_With_memory | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use this chain we have to create a list of messages to add the `human` and `ai` message mannually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_memory_with_ranker.invoke({\"history\":[HumanMessage(\"My Name is Long.\"),\n",
    "                                          AIMessage(\"Hello Long.\")],\n",
    "                               \"input\":\"What is my Name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableWithMessageHistory\n",
    "\n",
    "We can also use `RunnableWithMessageHistory`. It automatically append the message to the chat history.\n",
    "\n",
    "append to message history via:\n",
    "\n",
    "`input_messages_key`: the key of from input-dictionary should be consider as user_message to add to history\n",
    "\n",
    "`history_messages_key`: the key of from input-dictionary should be consider as list of human-ai chat history\n",
    "\n",
    "Later on:\n",
    "`output_messages_key`: the key from output-dictionary (output of the end of the chain) should be consider as ai_message to add to history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "rag_memory_with_ranker_chain = RunnableWithMessageHistory(\n",
    "    rag_memory_with_ranker, # the chain\n",
    "    get_session_history, # the function to get the history\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_memory_with_ranker_chain.with_config(configurable={\"retriever_kwargs\": {\"k\":5}}).invoke({\"input\":\"Hello\"},config={\"configurable\": {\"session_id\": \"demorag\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_memory_with_ranker_chain.with_config(configurable={\"retriever_kwargs\": {\"k\":5}}).invoke({\"input\":\"How do I make spaghetti carbonara?\"},config={\"configurable\": {\"session_id\": \"demorag\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_memory_with_ranker_chain.with_config(configurable={\"retriever_kwargs\": {\"k\":5}}).invoke({\"input\":\"Can you give me a detail about the recipe for 4 person?\"},config={\"configurable\": {\"session_id\": \"demorag\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_memory_with_ranker_chain.with_config(configurable={\"retriever_kwargs\": {\"k\":5}}).invoke({\"input\":\"What is self-rag about?\"},config={\"configurable\": {\"session_id\": \"demorag\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION LLM APPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "from langfuse import Langfuse\n",
    "load_dotenv()\n",
    "\n",
    "# if you run to error, replace the values with your own values, instead loading from enviroment variables.\n",
    "langfuse_handler = CallbackHandler(\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\"),\n",
    ")\n",
    "langfuse_client = Langfuse(    \n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\"),)\n",
    "\n",
    "def scoring_run(trace_id,metric_name,metric_value):\n",
    "    \"\"\"Scoring the langfuse-run.\"\"\"\n",
    "    langfuse_client.score(\n",
    "        trace_id=trace_id,\n",
    "        name=metric_name,\n",
    "        value=metric_value,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tracing run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_handler = CallbackHandler(\n",
    "    secret_key=\"sk-lf-ac81d8f4-4519-4905-bc6f-8965b78e0bcf\",\n",
    "    public_key=\"pk-lf-37e4e930-70ea-4688-8955-fc7381de892b\",\n",
    "    host=\"https://cloud.langfuse.com\",\n",
    ")\n",
    "langfuse_client = Langfuse(    \n",
    "    secret_key=\"sk-lf-ac81d8f4-4519-4905-bc6f-8965b78e0bcf\",\n",
    "    public_key=\"pk-lf-37e4e930-70ea-4688-8955-fc7381de892b\",\n",
    "    host=\"https://cloud.langfuse.com\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_memory_with_ranker_chain.with_config(configurable={\"retriever_kwargs\": {\"k\":5}}).invoke({\"input\":\"Hello.\"},\n",
    "                                                                                                config={\"configurable\": \n",
    "                                                                                                        {\"session_id\": \"demorag2\"},\n",
    "                                                                                                        \"callbacks\": [langfuse_handler]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_memory_with_ranker_chain.with_config(configurable={\"retriever_kwargs\": {\"k\":5}}).invoke({\"input\":\"How do I make spaghetti?\"},\n",
    "                                                                                                config={\"configurable\": \n",
    "                                                                                                        {\"session_id\": \"demorag2\"},\n",
    "                                                                                                        \"callbacks\": [langfuse_handler]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_memory_with_ranker_chain.with_config(configurable={\"retriever_kwargs\": {\"k\":5}}).invoke({\"input\":\"How do I use the recipe to make it for 4 person?\"},\n",
    "                                                                                                config={\"configurable\": \n",
    "                                                                                                        {\"session_id\": \"demorag2\"},\n",
    "                                                                                                        \"callbacks\": [langfuse_handler]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUTING RAG PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_questions = [\"How does Self-Reflective Retrieval-Augmented Generation (SELF-RAG) work?\",\n",
    "             \"What is RAPTOR and hwo does it works?\",\n",
    "             \"How can I improve my LLM on domain specific data such that it performs better than GPT-4?\",\n",
    "             \"How does Direct Preference Optimization (DPO) work?\",\n",
    "             \"What is the difference between RAG and LLM?\",\n",
    "             \"How can I evaluate a Large Language Model (LLM)?\",\n",
    "             \"Is there a way to build an LLM-agent for medical data?\"]\n",
    "dataset_name = \"dida-workshop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_client.create_dataset(\n",
    "    name=dataset_name,\n",
    "    # optional description\n",
    "    description=\"dataset for evaluating faithfullness of RAG system\",\n",
    "    # optional metadata\n",
    "    metadata={\n",
    "        \"author\": \"test-user\",\n",
    "        \"type\": \"benchmark demo\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in evaluation_questions:\n",
    "    langfuse_client.create_dataset_item(\n",
    "        dataset_name=dataset_name,\n",
    "        input={\n",
    "            \"question\": question\n",
    "        },\n",
    "        expected_output={\n",
    "        },\n",
    "        metadata={\n",
    "            \"info\": \"dataset contains only question.\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather all the components again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Context: ```{context}```\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\"\\\n",
    "                  \" If you don't know the answer, just say that you don't know.\"\\\n",
    "                \" You must answer the question using only the provided context and do not include any information from outside the given context.\"),\n",
    "    (\"human\",template)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Component\n",
    "This makes easier to test between models/components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name will change the name of the component\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",\n",
    "                 temperature=0,\n",
    "                 api_key=os.getenv(\"OPENAI_KEY\")).with_config(\n",
    "                     {\"run_name\": \"GPT-3.5\"}).configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    default_key=\"gpt-3.5\",\n",
    "    gpt4=ChatOpenAI(model=\"gpt-4o\", #gpt-4o is cheaper than gpt-4 turbo\n",
    "                      temperature=0,\n",
    "                      api_key=os.getenv(\"OPENAI_KEY\")).with_config(\n",
    "                          {\"run_name\": \"gpt-4o\"}),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate on on rag without memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = RunnableParallel({\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "}) | RunnableParallel({\n",
    "    \"retrieved_documents\": ranker,\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    })|RunnableParallel({\"context\": itemgetter(\"retrieved_documents\") | RunnableLambda(formatting_page_content),\n",
    "                         \"question\": itemgetter(\"question\"),\n",
    "                         \"retrieved_documents\":itemgetter(\"retrieved_documents\")}) | RunnableParallel({\"completion\": rag_prompt | llm | parser,\n",
    "                                                                                                       \"retrieved_documents\": itemgetter(\"retrieved_documents\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=rag_chain.with_config(configurable={\"llm\": \"gpt4\",\n",
    "                                      \"retriever_kwargs\": {\"k\":5}}).invoke(\"How does self-rag work?\",config={\"callbacks\": [langfuse_handler]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create completions from the dataset on langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime.now().strftime(\"_%Y-%m-%d_%H-%M-%S\")\n",
    "dataset = langfuse_client.get_dataset(dataset_name)\n",
    "ids = []\n",
    "contexts = []\n",
    "questions = []\n",
    "answers = []\n",
    "model_ids=[\"gpt-3.5\",\"gpt4\"]\n",
    "exp_time= datetime.now().strftime(\"_%Y-%m-%d_%H-%M-%S\")\n",
    "for model_id in model_ids:\n",
    "    for item in dataset.items:\n",
    "        store = {}\n",
    "        handler = item.get_langchain_handler(run_name=model_id+exp_time)\n",
    "        answer=rag_chain.with_config(configurable={\"llm\": model_id,\"retriever_kwargs\": {\"k\":5}}).invoke(\n",
    "            item.input[\"question\"],\n",
    "            config={\"callbacks\": [handler]})\n",
    "        \n",
    "        id = handler.get_trace_id()\n",
    "        ids.append(id)\n",
    "        answers.append(answer[\"completion\"])\n",
    "        questions.append(item.input[\"question\"])\n",
    "        contexts.append([d.page_content for d in answer[\"retrieved_documents\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USE RAGAS FOR EVALUATE RAG\n",
    "\n",
    "`faithfullness`: is the response supported by the retrieved context.\n",
    "\n",
    "`answer_relevancy`: is the response relevant to the query\n",
    "\n",
    "Both metrics does not requires groundtruth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = {\n",
    "    'question': questions,\n",
    "    'answer': answers,\n",
    "    'contexts' : contexts,\n",
    "    #'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n",
    "}\n",
    "\n",
    "eval_dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "score = evaluate(eval_dataset,metrics=[faithfulness,answer_relevancy])\n",
    "df = score.to_pandas()\n",
    "df[\"ids\"] = ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding score to each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    exp = df.iloc[i]\n",
    "    exp_id = exp[\"ids\"]\n",
    "    faithfullness = exp[\"faithfulness\"]\n",
    "    answer_relevancy = exp[\"answer_relevancy\"]\n",
    "    scoring_run(exp_id,\"faithfulness\",faithfullness)\n",
    "    scoring_run(exp_id,\"answer_relevance\",answer_relevancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BACKGROUND:\n",
    "\n",
    "Our chain always call the retrieval. We want the LLM\n",
    "\n",
    "- database related question: stay faithfull to the documents.\n",
    "\n",
    "- not related to the database question: do not use Retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_message = \"\"\"You are a helpful assistant. Your task is to decide to use information from a database to answer the question or not. The database contain information related to machine learning. \n",
    "\n",
    "Here are a list of short name of the paper in the database:\n",
    "- Direct Preference Optimization (DPO)\n",
    "- Self-Reflective Retrieval-Augmented Generation (SELF-RAG)\n",
    "- RAPTOR: Recursive Abstractive Processing For Tree-Organized Retrieval\n",
    "- Medagents: Large Language Models as Collaborators for Zero-shot Medical Reasoning\n",
    "- Replacing Judeges with Juries: Evaluating LLM Generations with a Panel of Diverse Models\n",
    "- Iterative Reasoning Preference Optimization (IRPO)\n",
    "- LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report\n",
    "\n",
    "Decide to use the retrieved document or not. return only one word `YES` or `NO`.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`router` chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(routing_message),\n",
    "    (\"human\",\"{question}\")]\n",
    ")\n",
    "router_chain = router_prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_chain.invoke(\"What is DPO?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_chain.with_config(configurable={\"llm\": \"gpt4\"}).invoke(\"What is DPO?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definie a router-function to select the `chain` base on its decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def routing_function(input_dict):\n",
    "    if \"YES\" in input_dict[\"decision\"]:\n",
    "        return itemgetter(\"question\")|rag_chain\n",
    "    return ChatPromptTemplate.from_template(\"{question}\")|llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_rag_chain = {\"decision\":router_chain,\"question\":RunnablePassthrough()}|RunnableLambda(routing_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_rag_chain.invoke(\"How does self-rag work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_rag_chain.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL RAG CHAIN WITH MEMORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect what we did"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUTER PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_message = \"\"\"You are a helpful assistant. Your task is to decide to use information from a database to answer the question or not. The database contain information related to machine learning. \n",
    "\n",
    "Here are a list of short name of the paper in the database:\n",
    "- Direct Preference Optimization (DPO)\n",
    "- Self-Reflective Retrieval-Augmented Generation (SELF-RAG)\n",
    "- RAPTOR: Recursive Abstractive Processing For Tree-Organized Retrieval\n",
    "- Medagents: Large Language Models as Collaborators for Zero-shot Medical Reasoning\n",
    "- Replacing Judeges with Juries: Evaluating LLM Generations with a Panel of Diverse Models\n",
    "- Iterative Reasoning Preference Optimization (IRPO)\n",
    "- LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Repor\n",
    "\n",
    "Decide to use the retrieved document or not. return only one word `YES` or `NO`.\"\"\"\n",
    "router_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(routing_message),\n",
    "    (\"human\",\"{question}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_template = \"\"\"Question: {question}\n",
    "\n",
    "Context: ```{context}```\n",
    "\"\"\"\n",
    "rag_prompt_messages = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\"\\\n",
    "                  \" If you don't know the answer, just say that you don't know.\"\\\n",
    "                \" You must answer the question using only the provided context and do not include any information from outside the given context.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\",rag_template)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA BOT PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt_messages = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(\"You are an helpful assistant. If you do not know the answer, just say that you don't know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\",\"{question}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_component = RunnableParallel({\n",
    "    \"documents\": itemgetter(\"question\")|retriever,\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    \"history\": itemgetter(\"history\"),\n",
    "})\n",
    "\n",
    "ranker_component = RunnableParallel({\n",
    "    \"retrieved_documents\": ranker,\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    \"history\": itemgetter(\"history\"),\n",
    "    })\n",
    "\n",
    "context_formatter_component = RunnableParallel({\"context\": itemgetter(\"retrieved_documents\") | RunnableLambda(formatting_page_content),\n",
    "                         \"question\": itemgetter(\"question\"),\n",
    "                         \"history\": itemgetter(\"history\"),\n",
    "                         \"retrieved_documents\":itemgetter(\"retrieved_documents\")})\n",
    "\n",
    "rag_llm_component = RunnableParallel({\"completion\": rag_prompt_messages | llm | parser,\n",
    "                                        \"retrieved_documents\": itemgetter(\"retrieved_documents\")})\n",
    "\n",
    "rag_chain_with_memory_placeholder = retriever_component | ranker_component | context_formatter_component | rag_llm_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_chain = router_prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a runnable with `completion` and `retrieved_documents` to make sure this chain return the same structure as `rag_chain_with_memory_placeholder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_llm_component = RunnableParallel({\"completion\":qa_prompt_messages|llm | parser,\n",
    "                                     \"retrieved_documents\":lambda x: []})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def routing_function(input_dict):\n",
    "    if \"YES\" in input_dict[\"decision\"]:\n",
    "        return rag_chain_with_memory_placeholder\n",
    "    return qa_llm_component\n",
    "workshop_chatbot = RunnableParallel({\"decision\":router_chain,\n",
    "                         \"question\":itemgetter(\"question\"),\n",
    "                         \"history\":itemgetter(\"history\")}) | RunnableLambda(routing_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need also \n",
    "\n",
    "`output_messages_key` because `RunnableLambda(routing_function)` return a dictionary with \n",
    "\n",
    "```python\n",
    "{\"completion\":...,\n",
    "\"retrieved_documents\":...\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the store we can also change `session_id`\n",
    "store = {}\n",
    "workshop_chatbot_with_memory = RunnableWithMessageHistory(\n",
    "    workshop_chatbot,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    "    output_messages_key=\"completion\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in workshop_chatbot_with_memory.stream({\"question\":\"Hello I am Long\"},config={\"configurable\": {\"session_id\": \"demorag\"}}):\n",
    "    print(x.get(\"completion\",\"\"), end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in workshop_chatbot_with_memory.stream({\"question\":\"How does Self-Rag work?\"},config={\"configurable\": {\"session_id\": \"demorag\"}}):\n",
    "    print(x.get(\"completion\",\"\"), end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in workshop_chatbot_with_memory.stream({\"question\":\"What is my name?\"},config={\"configurable\": {\"session_id\": \"demorag\"}}):\n",
    "    print(x.get(\"completion\",\"\"), end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in workshop_chatbot_with_memory.stream({\"question\":\"How do I make spaghetti for 4 person?\"},config={\"configurable\": {\"session_id\": \"demorag\"}}):\n",
    "    print(x.get(\"completion\",\"\"), end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
